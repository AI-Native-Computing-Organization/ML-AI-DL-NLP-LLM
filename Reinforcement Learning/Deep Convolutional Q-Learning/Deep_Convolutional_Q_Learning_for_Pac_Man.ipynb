{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ],
      "metadata": {
        "id": "EAiHVEoWHy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b605535-cee5-492c-fd94-229d021a5fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.2.1)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.7.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "H-wes4LZdxdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np #numpy\n",
        "import torch #pytorch\n",
        "import torch.nn as nn #importing neural networks\n",
        "import torch.optim as optim #importing optimizer\n",
        "import torch.nn.functional as F #to use functions\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "m7wa0ft8e3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "dlYVpVdHe-i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self,action_size, seed =42):\n",
        "    super(Network, self).__init__() #just to activate inheritence\n",
        "    self.seed = torch.manual_seed(seed) # just to generate some random vectors\n",
        "\n",
        "    #CNN layers\n",
        "    self.conv1=nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4)#as using rgb, we use input channel 3, we want 32 output channels,good kernel size is 8*8, good stride value is 4\n",
        "    #Bash normalization operation\n",
        "    self.bn1=nn.BatchNorm2d(32) #we had 32 channels of feature maps (output channel)\n",
        "\n",
        "    #three more series of a convolution followed by a batch normalization operations\n",
        "    #Note; input and output channels are going to increase but the kernel size and stride is going to decrease (as we are flattening the matrix)\n",
        "    #second layer\n",
        "    self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)# as connected to 1st layer, the input for this will be 32; also got 64 channels to be good for output here\n",
        "    self.bn2=nn.BatchNorm2d(64)\n",
        "    #3rd layer\n",
        "    self.conv3=nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)# 64 inputs from conv2 but 64 output as well because after experimenting, it's still gives good result\n",
        "    self.bn3=nn.BatchNorm2d(64)\n",
        "    #4th layer\n",
        "    self.conv4=nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1) #we wanna gradually increase the depths, meaning the number of channels over the convolutions. And so since we didn't do it for the previous one, well we're gonna do it here and we're gonna add 128 output channel\n",
        "    self.bn4=nn.BatchNorm2d(128)\n",
        "\n",
        "    #first full connection\n",
        "    self.fc1 = nn.Linear(10*10*128,512)#input feature will be the number of output features resulting from flattening all the previous convolutions.\n",
        "    \"\"\" input size minus the kernel size plus two times the padding, all that divided by the stride, and then plus one. So you would need to apply this formula here first to get the output size of the first convolutional layer. Then to apply this formula another time here\n",
        "    to get the output size of the second convolutional layer.Then another time here to get the output size of the third convolutional layer. And then finally another time here to get the output size of the fourth convolutional layer. Which is 10*10*128\n",
        "    After some experimentation and hyper parameter tuning is actually 512 artificial neurons or output features in this first fully connected layer resulting from this first full connection.\"\"\"\n",
        "    self.fc2 = nn.Linear(512,256) #here the input is the output of fc1= 512, and a good number of output neuron should be 256 after seen from various experiments\n",
        "    #final fully connected layer\n",
        "    self.fc3 = nn.Linear(256, action_size) #input is the output of fc2, and output will be the action size\n",
        "\n",
        "  def forward(self, state):\n",
        "    #forward propagate from image to first convolutional layer\n",
        "    x= F.relu(self.bn1(self.conv1(state)))#self.conv1(state) will forward the image to convolutional layer and then we pass this whole to batch normalization layer by self.bn1(); let's activate this using relu F.relu()\n",
        "    #forward from first convolutation layer to second\n",
        "    x= F.relu(self.bn2(self.conv2(x)))\n",
        "    #forward from second convolution layer to third\n",
        "    x= F.relu(self.bn3(self.conv3(x)))\n",
        "    #forward from third convolution layer to fourth\n",
        "    x= F.relu(self.bn4(self.conv4(x)))\n",
        "    #we just need to do a little reshape in order to reshape the tensor in order to flatten it\n",
        "    x=x.view(x.size(0),-1) #first dimension corresponding to the batch remains the same and the other dimensions are flattened\n",
        "\n",
        "    #let's take our signal x, then let's forward propagate this signal x from the final flattening layer, to the first fully connected layer,\n",
        "    x= F.relu(self.fc1(x)) # forward propagate to first fully connected layer fc1 using self.fc1(x) then activate it using F.relu\n",
        "    x= F.relu(self.fc2(x)) # forward propagate to second\n",
        "    x= self.fc3(x) # forward propagate to third\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6BSSi437JA-A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "rUvCfE_mhwo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "WWCDPF22lkwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym #importing gymnasium\n",
        "env = gym.make('MsPacmanDeterministic-v0',full_action_space=False) #creating the environment, full_action_space = false  basically ensure that the agent uses a simplified set of actions for Miss Pacman\n",
        "state_shape= env.observation_space.shape #responds to the rgb channels here\n",
        "state_size= env.observation_space.shape[0] #the number of elements in this input state.\n",
        "number_actions = env.action_space.n #number of actions\n",
        "print('State Shape', state_shape) #State Shape (210, 160, 3) rgb channels\n",
        "print('State size:', state_size)\n",
        "print('Number of actions', number_actions) #action state should be 5, but here it will show 9 because Miss Pacman deterministic actually contains more actions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaGd-r9KVh8w",
        "outputId": "c6af94e8-3938-4800-a620-dd94e8f85049"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Shape (210, 160, 3)\n",
            "State size: 210\n",
            "Number of actions 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate= 5e-4\n",
        "minibatch_size = 64\n",
        "discount_factor = 0.99\n",
        "#No need of soft update here and no need of replay buffer here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ACbccIKYEEb",
        "outputId": "32cb765c-5834-41ac-f97a-e925f903f81c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the frames"
      ],
      "metadata": {
        "id": "U2bDShIEkA5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to pre-process the frames so that the input images can be converted into PyTorch tensors that are accepted by the neural network of our AI\n",
        "from PIL import Image\n",
        "from torchvision import transforms #transforms module import\n",
        "\n",
        "def preprocess_frame(frame):#frames coming from the pacman game will be converted to pytorch tensors\n",
        "    #we have now one frame from the game which is in the format of numpy array. So, convert this to pil image object\n",
        "    frame=Image.fromarray(frame) #pil image object\n",
        "\n",
        "    #Preprocessing object\n",
        "    preprocess = transforms.Compose([transforms.Resize((128,128)),transforms.ToTensor()])#compose class takes list as an input and have resized dimension. We have shape (210, 160, 3) which is hard to process and we will resize this to 128 by 128 (128,128) using transforms.Resize(), then transforms.ToTensor() will convert them to pytorch tensors\n",
        "    return preprocess(frame).unsqueeze(0) #return the preprocessed frame\n",
        "    # our frames always need to be in their corresponding batch. And in order to keep track of which batch each frame belongs to,we're gonna use the unsqueeze method, which will just take as input one argument, which is the index of that dimension of the batch. using [0]so that the dimension of the batch will be the first dimension.\n",
        ""
      ],
      "metadata": {
        "id": "9FLC3vqmYlch"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the DCQN class"
      ],
      "metadata": {
        "id": "imMdSO-HAWra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for explanation of the code, check: https://hashnode.com/post/clzp1dd20000409jo3nmu5eat\n",
        "class Agent(): #creating our agent\n",
        "  def __init__(self,action_size): #no state_size here\n",
        "    self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size=action_size\n",
        "    self.local_qnetwork=Network(action_size).to(self.device) #new\n",
        "    self.target_qnetwork=Network(action_size).to(self.device)  #new\n",
        "    self.optimizer=optim.Adam( self.local_qnetwork.parameters(),lr=learning_rate)\n",
        "    self.memory=deque(maxlen=1000)#new\n",
        "    #no timesteps\n",
        "\n",
        "  #Step method: And this is a method that will store experiences and decide when to learn from them\n",
        "  def step(self,state,action,reward,next_state,done):\n",
        "    #push method and timestep codes removes\n",
        "\n",
        "    #REPLACEMENT OF Push method\n",
        "    state=preprocess_frame(state).to(self.device)  #preprocess state\n",
        "    next_state=preprocess_frame(next_state).to(self.device) #next preprocess state\n",
        "    self.memory.append((state,action,reward,next_state,done)) #adding to memory\n",
        "\n",
        "    if len(self.memory)> minibatch_size: #new\n",
        "        experiences= random.sample(self.memory,k=minibatch_size) #new\n",
        "        self.learn(experiences,discount_factor)\n",
        "\n",
        "\n",
        "  def act(self,state,epsilon=0.):\n",
        "    state= preprocess_frame(state).to(self.device) #new ; the state is now is an image rather than an input vector which we had in DQN\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values= self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "\n",
        "\n",
        "  def learn(self,experiences,discount_factor):\n",
        "    states,actions, rewards,next_states,dones = zip(*experiences) #new\n",
        "    #we can deal with states either by using vstacks( which can take numpy arrays or even torch) or, we can use torch.cat\n",
        "\n",
        "\n",
        "    #vstacks(option 1)\n",
        "    states=torch.from_numpy(np.vstack(states)).float().to(self.device) #new\n",
        "    actions=torch.from_numpy(np.vstack(actions)).long().to(self.device) #new\n",
        "    rewards=torch.from_numpy(np.vstack(rewards)).float().to(self.device) #new\n",
        "    next_states=torch.from_numpy(np.vstack(next_states)).float().to(self.device)  #new\n",
        "    dones=torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device) #new\n",
        "\n",
        "\n",
        "    #--> torch.cat (option 2)\n",
        "    #states = torch.cat(states)  #states and next_states are already pytorch tensors here\n",
        "    #next_states = torch.cat(next_states)\n",
        "\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (discount_factor * next_q_targets*(1-dones))\n",
        "    q_expected= self.local_qnetwork(states).gather(1,actions)\n",
        "    loss = F.mse_loss(q_expected,q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    #no soft update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMgkEtdtbjsE",
        "outputId": "b597abdd-0deb-41ff-9380-7bc2c9872536"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the DCQN agent"
      ],
      "metadata": {
        "id": "yUg95iBpAwII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(number_actions)"
      ],
      "metadata": {
        "id": "I34SO3UWqIxh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCQN agent"
      ],
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes= 2000\n",
        "max_number_timesteps_per_episode = 10000 #new; increased the eiposodes for better training\n",
        "\n",
        "epsilon_starting_value=1.0\n",
        "epsilon_ending_value= 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon= epsilon_starting_value\n",
        "\n",
        "\n",
        "scores_on_100_episodes= deque(maxlen=100)\n",
        "\n",
        "\n",
        "#main\n",
        "for episode in range(1,number_episodes+1):\n",
        "  state, _ = env.reset()\n",
        "  score=0\n",
        "  for t in range(max_number_timesteps_per_episode):\n",
        "    action= agent.act(state,epsilon)\n",
        "    next_state,reward,done,_,_= env.step(action)\n",
        "    agent.step(state,action,reward,next_state,done)\n",
        "    state=next_state\n",
        "    score+=reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon= max(epsilon_ending_value,epsilon_decay_value*epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode,np.mean(scores_on_100_episodes)), end=\"\")\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0: #new: if the average scores_on_100_episodes is larger than 500, well time to say we win\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode-100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azqg0I0iqTbA",
        "outputId": "2cce95d2-d44a-430c-cebb-e156e65aca1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 1\tAverage Score: 250.00\rEpisode 1\tAverage Score: 250.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "-0WhhBV8nQdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0') #only change is 'MsPacmanDeterministic-v0'\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}